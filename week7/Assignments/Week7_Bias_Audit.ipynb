{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Assignment: Part 3 - Practical Audit\n",
    "\n",
    "## Task: Audit a Dataset for Bias\n",
    "\n",
    "**Dataset:** COMPAS Recidivism Dataset.\n",
    "\n",
    "**Goal:**\n",
    "1. Use Python and `aif360` to analyze racial bias in risk scores.\n",
    "2. Generate visualizations.\n",
    "3. Write a 300-word report summarizing findings and remediation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install aif360 pandas matplotlib numpy requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We will download the COMPAS dataset directly and load it into a pandas DataFrame. Then we will convert it into an `aif360` `BinaryLabelDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the COMPAS dataset\n",
    "url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "response = requests.get(url)\n",
    "content = response.content\n",
    "df = pd.read_csv(io.BytesIO(content))\n",
    "\n",
    "# Preprocessing similar to ProPublica's analysis\n",
    "df = df[\n",
    "    (df[\"days_b_screening_arrest\"] <= 30) &\n",
    "    (df[\"days_b_screening_arrest\"] >= -30) &\n",
    "    (df[\"is_recid\"] != -1) &\n",
    "    (df[\"c_charge_degree\"] != \"O\") &\n",
    "    (df[\"score_text\"] != \"N/A\")\n",
    "]\n",
    "\n",
    "# Select relevant columns\n",
    "# race: Protected attribute\n",
    "# two_year_recid: Target variable (1 = recidivated, 0 = did not)\n",
    "# But usually for fairness we look at the *prediction* bias. \n",
    "# Here we are auditing the *dataset* itself (historical bias) or the *risk scores* if we treat them as predictions.\n",
    "# Let's analyze the bias in the ground truth (recidivism) vs race, \n",
    "# OR more commonly, we analyze the bias in the 'decile_score' or 'score_text' acting as a proxy for prediction.\n",
    "# For this audit, let's treat 'two_year_recid' as the label we want to predict, and check if the dataset itself has disparate impact.\n",
    "# OR we can treat 'score_text' (Low/Medium/High) as the prediction.\n",
    "# Let's create a binary prediction from score_text: High/Medium = 1 (High Risk), Low = 0 (Low Risk)\n",
    "\n",
    "df['high_risk'] = df['score_text'].apply(lambda x: 1 if x in ['High', 'Medium'] else 0)\n",
    "\n",
    "# We will analyze the bias in these 'high_risk' assignments.\n",
    "\n",
    "dataset_df = df[['race', 'high_risk']]\n",
    "\n",
    "# Filter for African-American and Caucasian for direct comparison\n",
    "dataset_df = dataset_df[dataset_df['race'].isin(['African-American', 'Caucasian'])]\n",
    "\n",
    "# Map race to binary: African-American = 0 (Unprivileged), Caucasian = 1 (Privileged)\n",
    "# Note: This mapping is arbitrary for the tool, but we must define who is 'privileged' in the context of the metric.\n",
    "# Usually, 'privileged' means the group that historically has the advantage (e.g., lower risk scores).\n",
    "dataset_df['race_binary'] = dataset_df['race'].apply(lambda x: 1 if x == 'Caucasian' else 0)\n",
    "\n",
    "# Create BinaryLabelDataset\n",
    "dataset = BinaryLabelDataset(\n",
    "    favorable_label=0, # 0 means Low Risk (Good outcome)\n",
    "    unfavorable_label=1, # 1 means High Risk (Bad outcome)\n",
    "    df=dataset_df,\n",
    "    label_names=['high_risk'],\n",
    "    protected_attribute_names=['race_binary']\n",
    ")\n",
    "\n",
    "privileged_groups = [{'race_binary': 1}] # Caucasian\n",
    "unprivileged_groups = [{'race_binary': 0}] # African-American\n",
    "\n",
    "print(\"Dataset loaded successfully.\")\n",
    "print(f\"Total records: {len(dataset_df)}\")\n",
    "print(dataset_df['race'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze Racial Bias\n",
    "\n",
    "We will calculate the **Disparate Impact** and **Mean Difference** (Statistical Parity Difference).\n",
    "\n",
    "*   **Disparate Impact:** Ratio of the rate of favorable outcome for the unprivileged group to the privileged group. Ideal is 1.0.\n",
    "*   **Mean Difference:** Difference in the rate of favorable outcomes. Ideal is 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = BinaryLabelDatasetMetric(\n",
    "    dataset,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "disparate_impact = metric.disparate_impact()\n",
    "mean_difference = metric.mean_difference()\n",
    "\n",
    "print(f\"Disparate Impact: {disparate_impact:.4f}\")\n",
    "print(f\"Mean Difference: {mean_difference:.4f}\")\n",
    "\n",
    "if disparate_impact < 0.8:\n",
    "    print(\"\\nWarning: Disparate impact is below 0.8, indicating significant bias against the unprivileged group.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations\n",
    "\n",
    "Let's visualize the proportion of individuals labeled as \"High Risk\" (Unfavorable) vs \"Low Risk\" (Favorable) for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rates\n",
    "priv_indices = (dataset_df['race_binary'] == 1)\n",
    "unpriv_indices = (dataset_df['race_binary'] == 0)\n",
    "\n",
    "priv_high_risk_rate = dataset_df[priv_indices]['high_risk'].mean()\n",
    "unpriv_high_risk_rate = dataset_df[unpriv_indices]['high_risk'].mean()\n",
    "\n",
    "print(f\"High Risk Rate (Caucasian): {priv_high_risk_rate:.2%}\")\n",
    "print(f\"High Risk Rate (African-American): {unpriv_high_risk_rate:.2%}\")\n",
    "\n",
    "# Plot\n",
    "groups = ['Caucasian', 'African-American']\n",
    "rates = [priv_high_risk_rate, unpriv_high_risk_rate]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(groups, rates, color=['blue', 'red'], alpha=0.7)\n",
    "plt.ylabel('Proportion Labeled High/Medium Risk')\n",
    "plt.title('Racial Bias in COMPAS Risk Scores')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(rates):\n",
    "    plt.text(i, v + 0.02, f\"{v:.1%}\", ha='center', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Audit Report\n",
    "\n",
    "### Findings\n",
    "\n",
    "The analysis of the COMPAS dataset reveals significant racial bias in the risk scores assigned to defendants. Specifically, we analyzed the 'score_text' variable, categorizing 'High' and 'Medium' scores as high-risk (unfavorable outcome) and 'Low' scores as low-risk (favorable outcome).\n",
    "\n",
    "The **Disparate Impact** metric is significantly below 1.0 (typically around 0.6-0.7 in this dataset), indicating that African-American defendants are far less likely to receive a 'Low Risk' score compared to Caucasian defendants. Conversely, the visualization shows that African-American defendants are labeled as 'High/Medium Risk' at a much higher rate than their Caucasian counterparts.\n",
    "\n",
    "This confirms that the COMPAS algorithm, or the data it relies on, disproportionately flags African-American defendants as potential recidivists. This disparity exists even when controlling for prior crimes in more detailed analyses, suggesting systemic bias in the underlying data or the model's weighting of features.\n",
    "\n",
    "### Remediation Steps\n",
    "\n",
    "To address this bias, several steps should be taken:\n",
    "\n",
    "1.  **Pre-processing (Reweighing):** We can use algorithms like `Reweighing` from the `aif360` toolkit to adjust the weights of the training examples. This would give more weight to favorable examples from the unprivileged group and less to unfavorable ones, helping to neutralize the bias in the training data before a model is built.\n",
    "2.  **In-processing (Adversarial Debiasing):** If training a new model, we could use adversarial debiasing techniques where the model tries to predict recidivism while simultaneously minimizing its ability to predict the protected attribute (race).\n",
    "3.  **Post-processing (Calibrated EqOdds):** We could adjust the decision thresholds for different groups to ensure that the False Positive Rates are equal across groups (Equal Opportunity), ensuring that innocent individuals from both groups are treated similarly.\n",
    "4.  **Transparency and Human Oversight:** The risk scores should not be used as the sole determinant for sentencing or bail. Judges must be educated about these biases and use the scores only as one of many factors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
