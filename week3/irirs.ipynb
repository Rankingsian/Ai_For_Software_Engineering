# Step 1: Install required packages
!pip install streamlit scikit-learn pandas numpy matplotlib seaborn streamlit-ace

# Step 2: Create and run the ML training script
%%writefile iris_training.py
import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report
from sklearn.impute import SimpleImputer
import pickle

def main():
    print("=== Iris Species Classification with Decision Tree ===\n")
    
    # Load data
    print("Loading Iris dataset...")
    iris = load_iris()
    X = iris.data
    y = iris.target
    feature_names = iris.feature_names
    target_names = iris.target_names
    
    print(f"Dataset loaded: {X.shape[0]} samples, {X.shape[1]} features")
    print(f"Features: {feature_names}")
    print(f"Target classes: {target_names}")
    
    # Handle missing values (though Iris dataset typically has none)
    print("\nChecking for missing values...")
    missing_values = np.isnan(X).sum()
    print(f"Missing values in each feature: {missing_values}")
    
    if missing_values.any():
        print("Imputing missing values...")
        imputer = SimpleImputer(strategy='mean')
        X = imputer.fit_transform(X)
    else:
        print("No missing values found!")
    
    # Split data
    print("\nSplitting data into training and testing sets...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"Training set: {X_train.shape[0]} samples")
    print(f"Testing set: {X_test.shape[0]} samples")
    
    # Train model
    print("\nTraining Decision Tree classifier...")
    clf = DecisionTreeClassifier(random_state=42, max_depth=3, min_samples_split=5)
    clf.fit(X_train, y_train)
    print("Model training completed!")
    
    # Evaluate model
    print("\nEvaluating model performance...")
    y_pred = clf.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    
    print("\nDetailed Classification Report:")
    print(classification_report(y_test, y_pred, target_names=target_names))
    
    # Save model
    print("\nSaving model...")
    model_data = {
        'model': clf,
        'feature_names': feature_names,
        'target_names': target_names
    }
    
    with open('iris_model.pkl', 'wb') as f:
        pickle.dump(model_data, f)
    
    print("Model saved as 'iris_model.pkl'")
    print("\n=== Model Training Completed ===")

if __name__ == "__main__":
    main()

# Step 3: Run the training script
!python iris_training.py

# Step 4: Create the Streamlit app
%%writefile app.py
import streamlit as st
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris

# Set page configuration
st.set_page_config(
    page_title="Iris Species Classifier",
    page_icon="üå∏",
    layout="wide"
)

# Load the trained model
@st.cache_resource
def load_model():
    try:
        with open('iris_model.pkl', 'rb') as f:
            model_data = pickle.load(f)
        return model_data
    except FileNotFoundError:
        st.error("Model file not found. Please run the training script first.")
        return None

def predict_species(model_data, features):
    model = model_data['model']
    prediction = model.predict([features])[0]
    probability = model.predict_proba([features])[0]
    return prediction, probability

def main():
    st.title("üå∏ Iris Species Classification App")
    st.markdown("""
    This app predicts the species of Iris flowers based on their measurements 
    using a trained Decision Tree classifier.
    """)
    
    model_data = load_model()
    if model_data is None:
        return
    
    st.sidebar.title("Navigation")
    app_mode = st.sidebar.selectbox("Choose the app mode", 
                                   ["Prediction", "Model Info", "Dataset Info"])
    
    if app_mode == "Prediction":
        show_prediction_interface(model_data)
    elif app_mode == "Model Info":
        show_model_info(model_data)
    elif app_mode == "Dataset Info":
        show_dataset_info()

def show_prediction_interface(model_data):
    st.header("üîÆ Make a Prediction")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("Enter Flower Measurements")
        
        sepal_length = st.slider("Sepal Length (cm)", 4.0, 8.0, 5.8, 0.1)
        sepal_width = st.slider("Sepal Width (cm)", 2.0, 4.5, 3.0, 0.1)
        petal_length = st.slider("Petal Length (cm)", 1.0, 7.0, 4.0, 0.1)
        petal_width = st.slider("Petal Width (cm)", 0.1, 2.5, 1.2, 0.1)
        
        features = [sepal_length, sepal_width, petal_length, petal_width]
        
        if st.button("Predict Species", type="primary"):
            prediction, probabilities = predict_species(model_data, features)
            species_name = model_data['target_names'][prediction]
            
            st.success(f"**Predicted Species: {species_name}**")
            
            st.subheader("Prediction Probabilities")
            prob_df = pd.DataFrame({
                'Species': model_data['target_names'],
                'Probability': probabilities
            })
            
            fig, ax = plt.subplots(figsize=(10, 6))
            bars = ax.bar(prob_df['Species'], prob_df['Probability'], 
                         color=['#ff9999', '#66b3ff', '#99ff99'])
            ax.set_ylabel('Probability')
            ax.set_title('Prediction Probabilities for Each Species')
            ax.set_ylim(0, 1)
            
            for bar, prob in zip(bars, prob_df['Probability']):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{prob:.2%}', ha='center', va='bottom')
            
            st.pyplot(fig)
    
    with col2:
        st.subheader("Feature Information")
        st.markdown("""
        **Feature Descriptions:**
        - **Sepal Length**: Length of sepal in cm
        - **Sepal Width**: Width of sepal in cm  
        - **Petal Length**: Length of petal in cm
        - **Petal Width**: Width of petal in cm
        
        **Species:**
        - Setosa
        - Versicolor  
        - Virginica
        """)
        
        st.info(f"""
        Current Input:
        - Sepal Length: {sepal_length} cm
        - Sepal Width: {sepal_width} cm
        - Petal Length: {petal_length} cm
        - Petal Width: {petal_width} cm
        """)

def show_model_info(model_data):
    st.header("ü§ñ Model Information")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Model Details")
        st.write("**Algorithm:** Decision Tree Classifier")
        st.write("**Library:** Scikit-learn")
        st.write(f"**Number of Features:** {len(model_data['feature_names'])}")
        st.write(f"**Number of Classes:** {len(model_data['target_names'])}")
        
        st.subheader("Feature Importance")
        feature_importance = model_data['model'].feature_importances_
        importance_df = pd.DataFrame({
            'Feature': model_data['feature_names'],
            'Importance': feature_importance
        }).sort_values('Importance', ascending=True)
        
        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.barh(importance_df['Feature'], importance_df['Importance'])
        ax.set_xlabel('Importance')
        ax.set_title('Feature Importance in Decision Tree')
        
        for bar, imp in zip(bars, importance_df['Importance']):
            width = bar.get_width()
            ax.text(width + 0.01, bar.get_y() + bar.get_height()/2.,
                   f'{width:.2%}', ha='left', va='center')
        
        st.pyplot(fig)
    
    with col2:
        st.subheader("Model Performance")
        st.markdown("""
        The model was evaluated on a held-out test set with the following metrics:
        
        - **Accuracy**: ~97%  
        - **Precision**: ~97%
        - **Recall**: ~97%
        
        **Training Details:**
        - Train-Test Split: 80-20
        - Random State: 42
        - Max Depth: 3
        - Min Samples Split: 5
        """)

def show_dataset_info():
    st.header("üìä Dataset Information")
    
    iris = load_iris()
    df = pd.DataFrame(iris.data, columns=iris.feature_names)
    df['species'] = iris.target
    df['species'] = df['species'].map({i: name for i, name in enumerate(iris.target_names)})
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Dataset Overview")
        st.write(f"**Total Samples:** {len(df)}")
        st.write(f"**Number of Features:** {len(iris.feature_names)}")
        st.write(f"**Number of Classes:** {len(iris.target_names)}")
        
        st.subheader("Class Distribution")
        species_count = df['species'].value_counts()
        fig1, ax1 = plt.subplots(figsize=(8, 6))
        ax1.pie(species_count.values, labels=species_count.index, autopct='%1.1f%%',
               colors=['#ff9999', '#66b3ff', '#99ff99'])
        ax1.set_title('Distribution of Iris Species')
        st.pyplot(fig1)
    
    with col2:
        st.subheader("Sample Data")
        st.dataframe(df.head(10))
        
        st.subheader("Feature Statistics")
        st.dataframe(df.describe())

if __name__ == "__main__":
    main()

# Step 5: Install localtunnel to expose the Streamlit app
!npm install -g localtunnel

# Step 6: Run Streamlit in the background and expose it
print("\nüöÄ Starting Streamlit app...")
print("Please wait a moment for the app to start...")

# Run streamlit in the background
import subprocess
import threading
import time

def run_streamlit():
    subprocess.run(["streamlit", "run", "app.py", "--server.port", "8501", "--server.address", "0.0.0.0"])

# Start streamlit in a separate thread
thread = threading.Thread(target=run_streamlit, daemon=True)
thread.start()

# Wait for streamlit to start
time.sleep(5)

print("üì± Your Streamlit app is starting...")
print("‚è≥ Please wait for the public URL to be generated...")

# Use localtunnel to expose the app
!npx localtunnel --port 8501 &

# Alternative: Using ngrok (uncomment if you prefer ngrok)
# !pip install pyngrok
# from pyngrok import ngrok
# public_url = ngrok.connect(8501)
# print(f"üéØ Your app is available at: {public_url}")

print("\nüîó The app should be available shortly...")
print("üìã Check the output above for the public URL")
print("‚è∞ It might take 30-60 seconds for the URL to become active")

# Step 7: Display the model file to verify it was created
print("\nüìÅ Checking if model file was created...")
!ls -la *.pkl

# Step 8: Show sample prediction using the model
print("\nüîç Making a sample prediction...")
import pickle
import numpy as np

try:
    with open('iris_model.pkl', 'rb') as f:
        model_data = pickle.load(f)
    
    # Sample prediction
    sample_features = [5.1, 3.5, 1.4, 0.2]  # Setosa example
    model = model_data['model']
    prediction = model.predict([sample_features])[0]
    probability = model.predict_proba([sample_features])[0]
    
    print(f"Sample prediction for features {sample_features}:")
    print(f"Predicted species: {model_data['target_names'][prediction]}")
    print("Probabilities:")
    for i, prob in enumerate(probability):
        print(f"  {model_data['target_names'][i]}: {prob:.2%}")
        
except FileNotFoundError:
    print("Model file not found. Please check if training completed successfully.")